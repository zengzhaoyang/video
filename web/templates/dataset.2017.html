<!DOCTYPE html>
<html lang="zh-CN" style="overflow-y: visible;">
  <head>
    <title>Microsoft Multimedia Challenge</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0" />
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link href="/static/css/bootstrap.min.css" rel="stylesheet" media="screen">
    <script src="/static/js/jquery.min.js"></script>
    <script src="/static/js/bootstrap.min.js"></script>
    <script src="/static/js/google-code-prettify/run_prettify.js"></script>
    <style type="text/css">
      .main {
        min-height: 100%;
      }

    </style>
    <script type="text/javascript">
      $(document).ready(function(){

      });
    </script>
  </head>
  <body>
    <nav class="navbar navbar-inverse">
      <div class="container">
        <div class="navbar-header">
<button class="navbar-toggle collapsed" data-toggle="collapse" data-target="#menu" aria-expanded="false">
            <span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/">
            <!-- <img src="http://mscoco.org/static/images/logo.jpg" height="155%"> -->
            Multimedia
          </a>
        </div>
<div class="collapse navbar-collapse" id="menu">
        <ul class="nav navbar-nav navbar-right">
          <li><a href="/">Home</a></li>
          <li><a href="/people">People</a></li>
          <li><a href="/challenge">Challenge</a></li>
          <li class="active"><a href="/dataset">Dataset</a></li>
          <li><a href="/contact">Contact</a></li>
        </ul>
      </div>
</div>
    </nav>

    <div class="container main">
      <div>

        <div class="col-md-12">
          <p>
            <!-- The dataset is based on MSR-VTT and we split the data according to 65%:30%:5% in the training, testing and validation set, respectively. -->
In the 2<sup>nd</sup> MSR Video to Language Challenge, we have combined the training set, validation set, and testing data in the 1<sup>st</sup> MSR Video to Language Challenge as the new training data. An additional test set of around 3K video clips will be released on June 1st as the final evaluation set. As such, we have 10K video clips for training and 3K video clips for testing this year. Each video is annotated with 20 natural sentences.
          </p>
          <p>
            * In MSR-VTT dataset, we provide the category information for each video clip and the video clip contains audio information as well. 
          </p>
          <p>
            All video info and caption sentences are formatted in a JSON file as
          </p>
          <pre class="prettyprint lang-py">
{
  "info" : {
    "year" : str,
    "version" : str,
    "description": str,
    "contributor": str,
    "data_created": str
  },
  "videos": {
    "id": int,
    "video_id": str,
    "category": int,
    "url": str,
    "start time": float,
    "end time": float,
    "split": str
  },
  "sentences": {
    "sen_id": int,
    "video_id": str,
    "caption": str
  }
}

          </pre>
<br>
<p>You can download video URLs and their associated sentences <b><a href="/static/resource/train_2017.zip">here</a></b>. For more details, please refer to <b><a href="http://staff.ustc.edu.cn/~xinmei/Project/Project.html">Prof. Xinmei Tian's Lab Page</a></b></p>
<p class="text-danger">Note that the testing data will ONLY be released to participants who have registered the challenge during the competition. Until the challenge completes, we will make the testing data publically available to the whole research community. </p>
        </div>

      </div>
    </div>
  </body>
</html>
